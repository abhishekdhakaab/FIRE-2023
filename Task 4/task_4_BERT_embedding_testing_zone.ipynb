{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zbij0cOvwe9",
        "outputId": "15bd2f5c-ddd8-443e-ebce-f96355d8ce64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-YmyADqv410",
        "outputId": "356bd5ca-1126-440f-a6c2-07ddadc41b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "model_name1='Kowsher/bangla-bert'\n",
        "model_name2='sagorsarker/bangla-bert-base'\n",
        "model_name3='csebuetnlp/banglabert_large'\n",
        "model_name4='l3cube-pune/bengali-bert'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name4)\n",
        "#model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "_V98O5Wlv7OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_test=pd.read_csv('/content/drive/MyDrive/hate rate/task 4/test dataset/test_A_AH_HASOC2023.csv')"
      ],
      "metadata": {
        "id": "sHI2Gz4e1TWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/MyDrive/hate rate/task 4/dataset/train_A_AH_HASOC2023.csv')"
      ],
      "metadata": {
        "id": "nbzkJlEdwwS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_assamese_text(text):\n",
        "    # Remove emojis and non-Assamese characters\n",
        "    clean_text = re.sub(r'[^\\w\\s\\u0980-\\u09FF]+', ' ', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
        "\n",
        "    return clean_text.strip()\n",
        "\n",
        "\n",
        "\n",
        "preprocessed_text=pd.Series()\n",
        "for indx,row in df.iterrows():\n",
        "  p_text=clean_assamese_text(row['text'])\n",
        "  preprocessed_text.at[indx]=p_text\n",
        "test_preprocessed_text=pd.Series()\n",
        "for indx,row in df_test.iterrows():\n",
        "  p_text=clean_assamese_text(row['text'])\n",
        "  test_preprocessed_text.at[indx]=p_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foLn4K24He8D",
        "outputId": "727dec16-a917-47dc-b86e-b4864dc8bb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-ad4ad6aa5ea2>:14: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  preprocessed_text=pd.Series()\n",
            "<ipython-input-44-ad4ad6aa5ea2>:18: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  test_preprocessed_text=pd.Series()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['pre_text']=preprocessed_text\n",
        "df_test['pre_text']=test_preprocessed_text"
      ],
      "metadata": {
        "id": "yHmCwcaFHjOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### these are set of all unique words that are there in the data"
      ],
      "metadata": {
        "id": "i84nPCPOHms7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sett_train=set()\n",
        "sett_test=set()\n",
        "for s in df['pre_text']:\n",
        "  words=s.split();\n",
        "\n",
        "  for word in words:\n",
        "    sett_train.add(word)\n",
        "for s in df_test['pre_text']:\n",
        "  words=s.split();\n",
        "  for word in words:\n",
        "    sett_test.add(word)"
      ],
      "metadata": {
        "id": "f3EoBB1KxGKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(sett_train)\n",
        "#print(sett_test)"
      ],
      "metadata": {
        "id": "vpaGOO8sIPiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### this part is if you want to see which word is there in AssameseBERT"
      ],
      "metadata": {
        "id": "xijSt0vdHsgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Get the BERT vocabulary\n",
        "bert_vocab = tokenizer.get_vocab()\n",
        "\n",
        "# Word you want to check\n",
        "target_word = \"example\"\n",
        "\n",
        "yes_set_train=set()\n",
        "no_set_train=set()\n",
        "yes_set_test=set()\n",
        "no_set_test=set()\n",
        "yes_set_test=set()\n",
        "for word in sett_train:\n",
        "  if word in bert_vocab:\n",
        "    yes_set_train.add(word)\n",
        "  else:\n",
        "    no_set_train.add(word)\n",
        "\n",
        "for word in sett_test:\n",
        "  if word in bert_vocab:\n",
        "    yes_set_test.add(word)\n",
        "  else:\n",
        "    no_set_test.add(word)"
      ],
      "metadata": {
        "id": "JN6IxvARwOnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train have total = ' ,len(sett_train),' token   & test have = ',len(sett_test))\n",
        "print('FOR TRAIN in vocab ={}   OOV= {} '.format(len(yes_set_train),len(no_set_train)))\n",
        "print('FOR TEST in vocab= {}   OOV= {} '.format(len(yes_set_test),len(no_set_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoRkjiIpODDl",
        "outputId": "6d073e8c-ba89-4cad-9c61-317e22e55d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train have total =  10465  token   & test have =  4070\n",
            "FOR TRAIN in vocab =3035   OOV= 7430 \n",
            "FOR TEST in vocab= 1612   OOV= 2458 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDbpP-gn2-R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQpax3nC2-PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_sRKbHB2-M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_H5LRIMp2-Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EqIWBfjg2-Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPjjpjH3xhrJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}