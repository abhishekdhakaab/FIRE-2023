{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd5K3zelVrQR"
      },
      "source": [
        "## for embedding (dont run multiple times it will take time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glXWIm94kdmy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le4AWu-nVgmK",
        "outputId": "a17981b0-d970-4143-867b-e7981777037c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-13 15:39:46--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2023-08-13 15:39:46--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2023-08-13 15:39:46--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  5.03MB/s    in 4m 45s  \n",
            "\n",
            "2023-08-13 15:44:31 (5.09 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Download and unzip the embeddings if using for 1st time. Y\n",
        "##You can also save and load from your specified path once you save in your folder\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip -q glove.twitter.27B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnTYp-UFVqck"
      },
      "outputs": [],
      "source": [
        "GLOVE_EMB_TRAINED = \"glove_emb_crf_weights.pkl\"\n",
        "GLOVE_VECTORIZER_TRAINED = \"glove_crf_tv_layer.pkl\"\n",
        "GLOVE_EMB=200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfaUjGB7V0y9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "glove_embeddings_path='/content/glove.twitter.27B.100d.txt'\n",
        "glove_embeddings_index={}\n",
        "with open(glove_embeddings_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        glove_embeddings_index[word] = coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtI80MtqS5vX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9970a94e-31f3-4448-c4ab-76e6b303dce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2951 sha256=9f5f12b9e4f8a2a3d90ad9438a84d6e542528bd9a18f6dff55823fe57cadb44a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post7\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=fa502e6b6ee2940f9038ffc4a9dfdf2ffca306d6518257a80b35064c7237a727\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "#!pip install numpy==1.19.5\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.2.0\n",
        "#!pip install tensorflow-addons==0.10.0\n",
        "!pip install tensorflow-addons\n",
        "!pip install sklearn scipy\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeRainAkS4Yl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import os\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoUyyePZRsMs",
        "outputId": "92e35e22-3e64-4ccf-f0c3-97a20a916930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tr3VNz8kjf3",
        "outputId": "8dd3a57e-ecc5-49f6-da39-249ea636d9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./drive/MyDrive/hate rate/task 4/needed file/pytorch_crf-0.7.2-py3-none-any.whl\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install \"/content/drive/MyDrive/hate rate/task 4/needed file/pytorch_crf-0.7.2-py3-none-any.whl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVCruPPvkmaF"
      },
      "outputs": [],
      "source": [
        "from torchcrf import CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA4vffztwRps"
      },
      "source": [
        "## part you can change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXW7yfo1YmSu",
        "outputId": "53547a37-515d-4f34-bbeb-a0afbacd7709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/crf.py': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#please upload the crf file before running the code\n",
        "!cp /content/crf.py . # Done for colab\n",
        "#!cp '/content/drive/MyDrive/hate rate/task 3/code (paper 1)/crf.py'\n",
        "from crf import CRF as CRF_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlcU_baibVXj"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/hate rate/task 3/dataset/train.csv',sep='|')\n",
        "\n",
        "\n",
        "N_CRF_TAGS = 3\n",
        "LSTM_UNITS = 512\n",
        "DENSE_UNITS = 50\n",
        "LSTM_DROPOUT = 0.2\n",
        "DENSE_DROPOUT = 0.2\n",
        "EPOCHS = 2 #(Default 5, check!)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "GLOVE_EMB = 100  # Assuming you're using GloVe embeddings\n",
        "GLOVE_EMBEDDING_DIM=100\n",
        "\n",
        "MAX_LEN=128\n",
        "# maximum word length is 112 and maximum tokenized size is 96 (because tokenizer will ignore punctuations of 112->96)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bi3BRs2jAjGk",
        "outputId": "1f875f16-8c3a-4539-de67-efa174479cbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id                                           sentence  \\\n",
              "0   0  Say it loud , say it clear , illegal #immigran...   \n",
              "1   1  Islam is not a religion, its just an excuse fo...   \n",
              "2   2  @user Care About Illegals Breaking U.S #Immigr...   \n",
              "3   3  Behold the future DACA America - unless we Man...   \n",
              "4   4  No that little tattoo of a flower on your ankl...   \n",
              "\n",
              "                                                span  \\\n",
              "0                   {'start': [8, 9], 'end': [8, 9]}   \n",
              "1                 {'start': [0, 10], 'end': [4, 13]}   \n",
              "2                       {'start': [40], 'end': [40]}   \n",
              "3               {'start': [10, 17], 'end': [14, 18]}   \n",
              "4  {'start': [14, 20, 25, 26], 'end': [14, 21, 25...   \n",
              "\n",
              "                                                 bio  \n",
              "0                      O O O O O O O O B I O O O O O  \n",
              "1                B I I I I O O O O O B I I I O O O O  \n",
              "2  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
              "3  O O O O O O O O O O B I I I I O O B I O O O O ...  \n",
              "4  O O O O O O O O O O O O O O B O O O O O B I O ...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-0448ebcf-ea34-44e3-97ca-d6392c9bdc8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>span</th>\n",
              "      <th>bio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Say it loud , say it clear , illegal #immigran...</td>\n",
              "      <td>{'start': [8, 9], 'end': [8, 9]}</td>\n",
              "      <td>O O O O O O O O B I O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Islam is not a religion, its just an excuse fo...</td>\n",
              "      <td>{'start': [0, 10], 'end': [4, 13]}</td>\n",
              "      <td>B I I I I O O O O O B I I I O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>@user Care About Illegals Breaking U.S #Immigr...</td>\n",
              "      <td>{'start': [40], 'end': [40]}</td>\n",
              "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Behold the future DACA America - unless we Man...</td>\n",
              "      <td>{'start': [10, 17], 'end': [14, 18]}</td>\n",
              "      <td>O O O O O O O O O O B I I I I O O B I O O O O ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>No that little tattoo of a flower on your ankl...</td>\n",
              "      <td>{'start': [14, 20, 25, 26], 'end': [14, 21, 25...</td>\n",
              "      <td>O O O O O O O O O O O O O O B O O O O O B I O ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0448ebcf-ea34-44e3-97ca-d6392c9bdc8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-e7d4a593-d72b-4271-b0d6-888aa09aeba5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7d4a593-d72b-4271-b0d6-888aa09aeba5')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-e7d4a593-d72b-4271-b0d6-888aa09aeba5 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0448ebcf-ea34-44e3-97ca-d6392c9bdc8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0448ebcf-ea34-44e3-97ca-d6392c9bdc8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IyScbRxFqdP",
        "outputId": "b4ae65dc-34bb-4db2-ac70-1acefb5d7b1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'sentence', 'span', 'bio'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWvVV723x_uz"
      },
      "source": [
        "# code starts here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uNFw7NOY-dY"
      },
      "outputs": [],
      "source": [
        "maxx=0\n",
        "sent=''\n",
        "for s in df['sentence']:\n",
        "  s_len=len(s.split())\n",
        "  if(s_len>maxx):\n",
        "    sent=s\n",
        "    maxx=s_len\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4D6qTYEyCcX",
        "outputId": "2ef2b6f8-b4f5-41db-ac3c-89791b1967f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112\n",
            "What I do know is that I live on a small Island and it seems that a lot of people from various countries in the world are willing to desert their homelands and their families just to come here , in my mind that tells me a lot about said people and their home countries , most of all it highlights their weaknesses of how their own countries have let them down and how they feel the need to feed off somebody elses country , how should we English interpret this without sounding rude ? ? ? , locust ? ? ? , parasite ? ? ? , you tell me . \n"
          ]
        }
      ],
      "source": [
        "print(maxx)\n",
        "print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLstXMBuZIDI"
      },
      "outputs": [],
      "source": [
        "tag2idx={'O':0,'B':1,\"I\":2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIMHDXKMYvw4",
        "outputId": "843649fd-0d2b-4a65-cf17-441460548f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e14f3cbb4249>:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  new_bio=pd.Series()\n"
          ]
        }
      ],
      "source": [
        "new_bio=pd.Series()\n",
        "for indx,data in df.iterrows():\n",
        "  arr=data['bio'].split()\n",
        "  arr=[tag2idx[x] for x in arr]\n",
        "  new_bio.at[indx]=arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx9JmWsfYqVW"
      },
      "outputs": [],
      "source": [
        "df['new_bio']=new_bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz2TJWycvi_W"
      },
      "outputs": [],
      "source": [
        "train_df,test_df=train_test_split(df,test_size=0.2,random_state=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kCdHM6-Y23d"
      },
      "outputs": [],
      "source": [
        "#for training data\n",
        "train_y = keras.preprocessing.sequence.pad_sequences(maxlen=MAX_LEN,\n",
        "                                               sequences=train_df['new_bio'],\n",
        "                                               padding=\"post\",\n",
        "                                               value=0)\n",
        "#train_y = [keras.utils.to_categorical(i, num_classes=N_CRF_TAGS) for i in train_y]\n",
        "train_y=np.asarray(train_y)\n",
        "\n",
        "# for test data\n",
        "test_y = keras.preprocessing.sequence.pad_sequences(maxlen=MAX_LEN,\n",
        "                                               sequences=test_df['new_bio'],\n",
        "                                               padding=\"post\",\n",
        "                                               value=0)\n",
        "#test_y = [keras.utils.to_categorical(i, num_classes=N_CRF_TAGS) for i in test_y]\n",
        "test_y=np.asarray(test_y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yven4c88pOqS",
        "outputId": "da6c0944-ff57-46ed-c3c7-1978a72a39e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1936, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMggfjbd6Bwl",
        "outputId": "d633f544-1198-48dc-9429-533186bd389e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "yes\n"
          ]
        }
      ],
      "source": [
        "for x in train_y[1]:\n",
        "  if(x==1):\n",
        "    print('yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwBcntGDkDUz"
      },
      "outputs": [],
      "source": [
        "GLOVE_EMBEDDING_DIM=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVed_QHEz0mB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyURR8Yr9Yor"
      },
      "outputs": [],
      "source": [
        "# Step 1: Fit the tokenizer on the sentence corpus\n",
        "tokenizer = Tokenizer(filters='',oov_token='[OOV]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAFTC1BTYHFe"
      },
      "outputs": [],
      "source": [
        "GLOVE_EMB=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPdUt0mrz_ia"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['sentence'].tolist())\n",
        "train_sequence=tokenizer.texts_to_sequences(train_df['sentence'].values)\n",
        "test_sequence=tokenizer.texts_to_sequences(test_df['sentence'].values)\n",
        "\n",
        "\n",
        "num_tokens=len(tokenizer.word_index)\n",
        "\n",
        "num_tokens=len(tokenizer.word_index)+1\n",
        "\n",
        "GLOVE_EMBEDDING_DIM=100\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr3mtfkyZgkW",
        "outputId": "7c6bc0ee-d387-495f-9f55-c4792a05952d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of words out of oov  1393\n",
            "number of words in vocabulary is  5645\n"
          ]
        }
      ],
      "source": [
        "oov_count=0\n",
        "not_oov=0\n",
        "\n",
        "#for train data\n",
        "from pandas._libs.lib import is_float_array\n",
        "embedding_matrix=np.zeros((num_tokens,GLOVE_EMBEDDING_DIM))\n",
        "for word,i in tokenizer.word_index.items():\n",
        "  embedding_vector=glove_embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "     not_oov+=1\n",
        "     embedding_matrix[i]=embedding_vector\n",
        "  else:\n",
        "        oov_count+=1\n",
        "        # Words not found in embedding index will be have random embedding.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        random_num1 = np.random.rand(GLOVE_EMB)\n",
        "        random_num2 = np.random.rand(GLOVE_EMB)\n",
        "        embedding_vector = [\n",
        "            r1 if r2 < 0.5 else -1 * r1\n",
        "            for r1, r2 in zip(random_num1, random_num2)\n",
        "        ]\n",
        "        # print(embedding_vector)\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print('the number of words out of oov ',oov_count)\n",
        "print('number of words in vocabulary is ',not_oov)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7pQyum_ecOI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2TSNHCkaTx1"
      },
      "outputs": [],
      "source": [
        "maxx_tokenized=0\n",
        "for i in range(len(train_sequence)):\n",
        "  current=len(train_sequence[i])\n",
        "  maxx_tokenized=max(maxx_tokenized,current)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VezsAhd2avJR",
        "outputId": "656cc0b9-74ad-4a6f-9717-ca02ad906762"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "maxx_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovsfa99PehPN",
        "outputId": "9aad7150-2629-45a4-8114-fe313ba79afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What I do know is that I live on a small Island and it seems that a lot of people from various countries in the world are willing to desert their homelands and their families just to come here , in my mind that tells me a lot about said people and their home countries , most of all it highlights their weaknesses of how their own countries have let them down and how they feel the need to feed off somebody elses country , how should we English interpret this without sounding rude ? ? ? , locust ? ? ? , parasite ? ? ? , you tell me . \n",
            "112\n"
          ]
        }
      ],
      "source": [
        "train_maxx=0\n",
        "sent=''\n",
        "for s in train_df['sentence']:\n",
        "  s_len=len(s.split())\n",
        "  if(s_len>train_maxx):\n",
        "    sent=s\n",
        "    train_maxx=s_len\n",
        "\n",
        "print(sent)\n",
        "print(len(sent.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyNO-ne4exc-"
      },
      "source": [
        "so this time the number of token is same as the number of word/punctuation/speical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSkt1xPBZiqk"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_padded_sequences = pad_sequences(train_sequence,padding='post', maxlen=MAX_LEN)\n",
        "test_padded_sequences = pad_sequences(test_sequence,padding='post', maxlen=MAX_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking padding index value\n",
        "a=train_padded_sequences[:2]\n",
        "b=a!=0\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idky82FOnPDK",
        "outputId": "b03019dd-3406-4683-df99-b64ae1c7cac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "   True  True  True False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False]\n",
            " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "   True  True  True  True  True  True  True  True  True  True  True False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_MvsGJ4foDz",
        "outputId": "14aa32b6-55e6-4a8a-dd0d-4079af760e27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1936, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXubQlu-e8WY",
        "outputId": "51254cd1-2e93-489e-d438-8db95c6ca800"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1936, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "train_padded_sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHvTc3Y9gAUc",
        "outputId": "d38be277-4dde-404f-e534-32a0b5352e4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "hB4bNdVc0j4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkaccsO6k8JK"
      },
      "outputs": [],
      "source": [
        "embedding_matrix=torch.tensor(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33cKxLgil2l4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnNWgFIrkOg5"
      },
      "outputs": [],
      "source": [
        "# #USING TORCH\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torchcrf import CRF\n",
        "\n",
        "# # Assuming you have defined MAX_LEN, GLOVE_EMB, embedding_matrix, LSTM_UNITS, LSTM_DROPOUT, DENSE_UNITS, N_CRF_TAGS\n",
        "\n",
        "# # Define the model architecture\n",
        "# class our_model(nn.Module):\n",
        "#     def __init__(self, num_tokens, num_tags, embedding_dim, lstm_units, lstm_dropout, dense_units, embedding_matrix):\n",
        "#         super(our_model, self).__init__()\n",
        "#         self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "#         self.bilstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_units, batch_first=True, bidirectional=True)\n",
        "#         self.bilstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_units, batch_first=True, bidirectional=True)\n",
        "#         self.dense1 = nn.Linear(2 * lstm_units, dense_units)\n",
        "#         self.dense2 = nn.Linear(dense_units, num_tags)\n",
        "#         self.crf = CRF(num_tags)\n",
        "\n",
        "#     def forward(self, input_text,tags):\n",
        "#         input_text=input_text.to(device)\n",
        "#         tags=tags.to(device)\n",
        "#         mask = input_text != 0\n",
        "#         mask = mask.permute(1, 0)\n",
        "#         # PAD_TOKEN=0\n",
        "#         # mask = torch.ne(input_text, PAD_TOKEN)\n",
        "#         print(mask.shape)\n",
        "#         x = self.embedding(input_text)\n",
        "#         x1, _ = self.bilstm1(x)\n",
        "#         x2, _ = self.bilstm2(x)\n",
        "#         x = x1 + x2  # residual connection to the first biLSTM\n",
        "#         x = self.dense1(x)\n",
        "#         x = self.dense2(x)\n",
        "#         x=x.permute(1,0,2)\n",
        "\n",
        "#         out = self.crf.decode(x,tags,mask=mask)\n",
        "#         print('out is ',out)\n",
        "#         return out\n",
        "\n",
        "# # Instantiate the model with the provided embedding_matrix\n",
        "# num_tokens = len(embedding_matrix)\n",
        "# num_tags = N_CRF_TAGS\n",
        "# embedding_dim = GLOVE_EMB\n",
        "# lstm_units = LSTM_UNITS\n",
        "# lstm_dropout = LSTM_DROPOUT\n",
        "# dense_units = DENSE_UNITS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#USING TORCH\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "\n",
        "# Assuming you have defined MAX_LEN, GLOVE_EMB, embedding_matrix, LSTM_UNITS, LSTM_DROPOUT, DENSE_UNITS, N_CRF_TAGS\n",
        "\n",
        "# Define the model architecture\n",
        "class our_model(nn.Module):\n",
        "    def __init__(self, num_tokens, num_tags, embedding_dim, lstm_units, lstm_dropout, dense_units, embedding_matrix):\n",
        "        super(our_model, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
        "        self.bilstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.bilstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=lstm_units, batch_first=True, bidirectional=True)\n",
        "        self.dense1 = nn.Linear(2 * lstm_units, dense_units)\n",
        "        self.dense2 = nn.Linear(dense_units, num_tags)\n",
        "        self.crf = CRF(num_tags)\n",
        "\n",
        "    def forward(self, input_text,tags):\n",
        "        input_text=input_text.to(device)\n",
        "        if(tags is not None):\n",
        "          tags=tags.to(device)\n",
        "        x = self.embedding(input_text)\n",
        "        x1, _ = self.bilstm1(x)\n",
        "        print()\n",
        "        x2, _ = self.bilstm2(x)\n",
        "        x = x1 + x2  # residual connection to the first biLSTM\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x=x.permute(1,0,2)\n",
        "        print('here')\n",
        "        if tags is not None:\n",
        "            print('inside if')\n",
        "            out = self.crf(x, tags)\n",
        "            return out\n",
        "        else:\n",
        "            print('have tags')\n",
        "            # During inference, use viterbi_decode to get predicted tags\n",
        "            x = x.permute(1, 0, 2)  # Transpose for CRF layer\n",
        "            predicted_tags = self.crf.decode(x)\n",
        "            return predicted_tags\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with the provided embedding_matrix\n",
        "num_tokens = len(embedding_matrix)\n",
        "num_tags = N_CRF_TAGS\n",
        "embedding_dim = GLOVE_EMB\n",
        "lstm_units = LSTM_UNITS\n",
        "lstm_dropout = LSTM_DROPOUT\n",
        "dense_units = DENSE_UNITS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CAf1OOG-tp78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGnxRg-Wl3V-"
      },
      "outputs": [],
      "source": [
        "# Check if a GPU device is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDF_S93-l3yV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming you have tokenized and converted text data as a list of token indices\n",
        "# Example: tokenized_data = [[2, 45, 6, 32], [10, 8, 20, 0], [15, 7, 9, 3, 1]]\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tokenized_data, targets):\n",
        "        self.data = tokenized_data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "# Sample targets (labels), assuming you have them in a separate list or array\n",
        "# Example: targets = [1, 0, 1]\n",
        "\n",
        "# Assuming you already have padded sequences for train_y and targets\n",
        "# Convert them to PyTorch tensors\n",
        "train_input = torch.tensor(train_padded_sequences, dtype=torch.long)\n",
        "targets = torch.tensor(train_y, dtype=torch.long)\n",
        "\n",
        "# Define your DataLoader\n",
        "batch_size = 32  # Adjust this based on your available memory and batch size preference\n",
        "train_dataset = CustomDataset(train_input, targets)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPQx3gyjl4zZ",
        "outputId": "a9767dec-c4a0-47d2-f51a-3eb3af6d0996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-122-6b4394b0f5d9>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "our_model(\n",
              "  (embedding): Embedding(7039, 100)\n",
              "  (bilstm1): LSTM(100, 512, batch_first=True, bidirectional=True)\n",
              "  (bilstm2): LSTM(100, 512, batch_first=True, bidirectional=True)\n",
              "  (dense1): Linear(in_features=1024, out_features=50, bias=True)\n",
              "  (dense2): Linear(in_features=50, out_features=3, bias=True)\n",
              "  (crf): CRF(num_tags=3)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load the model architecture\n",
        "# Load the model architecture\n",
        "# model_architecture_path = \"/content/drive/MyDrive/Research/hate speech/model weights/bert_same_lstm_two_dense_two_mbert__epoch_40/model_architecture.pth\"\n",
        "\n",
        "model = our_model(num_tokens, num_tags, embedding_dim, lstm_units, lstm_dropout, dense_units, embedding_matrix)\n",
        "\n",
        "\n",
        "# # # Load the model weights\n",
        "#\n",
        "# if(want_to_use_trained_model_which_is_saved_in_drive_for_this_code):\n",
        "#   model_weights_path = where_is_the_model_saved_that_model_weight_file_location\n",
        "#   model.load_state_dict(torch.load(model_weights_path))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZKwPn8xnl4wT",
        "outputId": "4d644df8-2964-4183-b69c-2e7d6c52fae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.9229, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4681.5674, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.0029, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.3965, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.1465, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.8276, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.5688, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.0195, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.2148, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.3330, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.5107, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4680.6055, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.2021, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.7578, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.6162, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.5498, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.0186, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.2363, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.4990, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.7856, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.2920, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.3516, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.3184, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.6235, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.0825, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.3496, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.8418, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4672.1353, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4673.3389, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.5957, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.7051, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.8809, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.9297, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4680.4395, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.0889, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4670.6562, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.9287, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.7979, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.8477, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.1318, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.0342, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.4717, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4681.8008, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.1934, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.9321, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.4331, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.8931, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.2949, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.7334, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.4355, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4681.9443, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.0674, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.5557, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.9580, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4683.5498, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.1064, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4673.9014, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.2773, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.6816, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.7852, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-2337.1931, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.1060, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.2476, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.0464, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4685.4521, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.0386, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4673.1548, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4673.1846, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4672.7852, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4670.5308, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4678.1387, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4681.1113, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4680.4502, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.9912, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.5430, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4682.2437, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4680.6118, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4673.5996, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4683.5498, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.7314, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.7275, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.2217, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4687.8311, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4680.8975, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.1260, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4674.8613, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4684.9521, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4675.9238, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4679.0166, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.4766, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4676.7881, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n",
            "y pred is  tensor(-4677.2534, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "\n",
            "here\n",
            "inside if\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-18227738187f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Transpose the logits to the shape (seq_length, batch_size, num_tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y pred is '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#loss = loss_fn(y_pred, y_true)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-6b4394b0f5d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_text, tags)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inside if'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# shape: (batch_size,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m# shape: (batch_size,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_compute_score\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;31m# Emission score for next tag, only added if next timestep is valid (mask == 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# shape: (batch_size,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0memissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# End transition score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Train the model\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=0.001)\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=1, verbose=True)\n",
        "model.train()\n",
        "\n",
        "#model training\n",
        "n_epochs = 2\n",
        "loss_fn = nn.BCELoss()\n",
        "total_data=len(train_df)\n",
        "prev_total_loss = None  # Variable to store the previous total_loss\n",
        "best_accuracy=0\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0\n",
        "    total_samples_train = 0\n",
        "    batch_c = 0\n",
        "    correct_predictions_train = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_inputs, batch_targets in train_dataloader:\n",
        "        batch_c += 1\n",
        "\n",
        "        batch_inputs = batch_inputs.long()\n",
        "        #y_true = batch['label'].unsqueeze(1).float().to(device)\n",
        "        # Transpose the logits to the shape (seq_length, batch_size, num_tags)\n",
        "        tags = batch_targets.permute(1, 0)\n",
        "        y_pred = model(batch_inputs,tags)\n",
        "\n",
        "        loss = loss_fn(y_pred, y_true)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        predicted_labels_train = (y_pred >= threshold).float()\n",
        "\n",
        "        total_samples_train += len(y_true)\n",
        "        for j in range(len(predicted_labels_train)):\n",
        "            if y_true[j] == predicted_labels_train[j]:\n",
        "                correct_predictions_train += 1\n",
        "\n",
        "    total_loss /= total_data\n",
        "\n",
        "    # # Check if the total_loss has changed by less than 2\n",
        "    # if prev_total_loss is not None and abs(total_loss - prev_total_loss) < 0.005:\n",
        "    #     lr_scheduler.step(total_loss)\n",
        "\n",
        "\n",
        "    prev_total_loss = total_loss\n",
        "\n",
        "    correct_predictions_val = 0\n",
        "    total_samples_val = 0\n",
        "\n",
        "    for val_batch in val_data_loader:\n",
        "        text_input = val_batch['text_input']\n",
        "\n",
        "        y_true = val_batch['label'].unsqueeze(1).float().to(device)\n",
        "        y_pred = model(text_input)\n",
        "\n",
        "        predicted_labels = (y_pred >= threshold).float()\n",
        "        total_samples_val += len(y_true)\n",
        "\n",
        "        for j in range(len(predicted_labels)):\n",
        "            if y_true[j] == predicted_labels[j]:\n",
        "                correct_predictions_val += 1\n",
        "    print(\"**********************\")\n",
        "    print('Epoch:', epoch)\n",
        "    validation_accuracy=(correct_predictions_val / total_samples_val) * 100\n",
        "    val_accuracy=(correct_predictions_val / total_samples_val) * 100\n",
        "    print('Val Accuracy:', val_accuracy)\n",
        "    train_accuracy=(correct_predictions_train / total_samples_train) * 100\n",
        "    print('Train Accuracy:', train_accuracy)\n",
        "    learning_rate=optimizer.param_groups[0]['lr']\n",
        "    print('Current Learning Rate:', learning_rate)\n",
        "    print('Total Loss:', total_loss)\n",
        "\n",
        "    new_row={'epoch':epoch,'val accuracy':val_accuracy,'train accuracy':train_accuracy,'loss':total_loss,'learning rate':learning_rate}\n",
        "    model_stats=model_stats.append(new_row,ignore_index=True)\n",
        "    if ( validation_accuracy > best_accuracy and (want_to_save_trained_model == True)) :\n",
        "        best_accuracy = validation_accuracy\n",
        "        torch.save(model.state_dict(), location1)\n",
        "        print(f\"Saved the best model with validation accuracy: {best_accuracy:.2f}\")\n",
        "        torch.save(model, location2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhurcG3i0PEU"
      },
      "source": [
        "to do : 1. cuda\n",
        "2. testing and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePk4fvpol4sb"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the optimizer and other settings\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=0.001)\n",
        "# Define other hyperparameters and settings for training\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    total_loss = 0\n",
        "    total_samples_train = 0\n",
        "    correct_predictions_train = 0\n",
        "\n",
        "    for batch_inputs, batch_targets in train_dataloader:\n",
        "        batch_inputs = batch_inputs.long()\n",
        "\n",
        "        # Transpose the tags to the shape (seq_length, batch_size)\n",
        "        tags = batch_targets.permute(1, 0)\n",
        "\n",
        "        # Forward pass: Compute the CRF negative log-likelihood loss\n",
        "        out = model(batch_inputs, tags)\n",
        "\n",
        "        # Compute the mean loss value across the batch\n",
        "        loss = out.mean()\n",
        "\n",
        "        # Backward pass: Compute gradients and update model parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_samples_train += batch_inputs.size(0)  # Batch size\n",
        "        # Calculate accuracy or other metrics if needed\n",
        "\n",
        "    # Compute and print average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Once training is done, you can use the model for inference and prediction as before\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNH7KRKPl4j_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unWoldhLRaLA"
      },
      "outputs": [],
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKqR6dLEyw2J",
        "outputId": "aaa9293b-fdb4-49ee-9146-c8180b512b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 4s 256ms/step\n"
          ]
        }
      ],
      "source": [
        "# Get the model's predictions on the test data\n",
        "predictions = model.predict(test_padded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNCMjyy4yx1n"
      },
      "outputs": [],
      "source": [
        "predictions.shape\n",
        "predicted_classes = tf.argmax(predictions, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp4BZOW0zT6G",
        "outputId": "3149def8-f952-4bb6-9387-6cf032ee8568"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(485, 128), dtype=int64, numpy=\n",
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])>"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sr-JpsTzmye"
      },
      "outputs": [],
      "source": [
        "test_y.shape\n",
        "true_classes = tf.argmax(test_y, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt2XBE8hxgCz",
        "outputId": "749231cb-75a3-4c32-f025-8a6978a1867c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9678157216494846\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the accuracy manually using a loop\n",
        "total_samples = len(test_y)\n",
        "correct_predictions = 0\n",
        "# Calculate the accuracy for each token for all samples\n",
        "accuracies_per_token = np.mean(predicted_classes == true_classes, axis=0)\n",
        "\n",
        "# Calculate the average accuracy across all tokens\n",
        "average_accuracy = np.mean(accuracies_per_token)\n",
        "\n",
        "print(average_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-EE11_GUKf3",
        "outputId": "c08706ec-ff2b-416d-9ccc-8c353a379bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 3s 170ms/step - loss: 0.1060 - accuracy: 0.9678\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "# X_test_tensor = tf.convert_to_tensor(X_test)\n",
        "# y_test_tensor = tf.convert_to_tensor(y_test)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "results = model.evaluate(test_padded_sequences, test_y)\n",
        "test_accuracy = results[1]\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km_c1ta4UKsA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpnjkex_9HxW"
      },
      "outputs": [],
      "source": [
        "# Step 1: Fit the tokenizer on the sentence corpus\n",
        "tokenizer = Tokenizer(filters='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DijAtQOA9H-T"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\"hey how are you hi there \"]\n",
        "\n",
        "\n",
        "#tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Step 2: Tokenization (word and punctuation)\n",
        "out = tokenizer.texts_to_sequences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmM6B5ly9JgR",
        "outputId": "fa8eb113-282f-4860-af2b-c0cb322dd59c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(out[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkKXQxRV9LWo"
      },
      "outputs": [],
      "source": [
        "train_sent=train_df['sentence'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv7zjigS-Wq-",
        "outputId": "5669d57c-4ff3-4ddb-be3d-77a5bca5a75f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzo2vdbYXQKz"
      },
      "outputs": [],
      "source": [
        "tokenizer=Tokenizer(filters=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPZ1eftu-ZP1",
        "outputId": "829df643-39b5-4c4e-ebd5-e4faeda37943"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1936"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ypt7cDyM-blk"
      },
      "outputs": [],
      "source": [
        "a=train_sent[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuSqTAEhXWNX"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXtitjjm-nz-",
        "outputId": "4cf7fd14-ee0c-4d9a-91c2-c6f9ab703792"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "bF-SsFXlXEL2",
        "outputId": "bb1f11fa-9ea0-44c2-9cfc-95ce431d5768"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Are there any whites left in that craphole or have they all been killed ?'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMeg27nF-tWY"
      },
      "outputs": [],
      "source": [
        "a=f\"{a}\"\n",
        "b=[\"Are there any whites left in that craphole or have they all been killed ?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkLvXE1x-pG_",
        "outputId": "ae42a032-c82e-450b-f3ee-9095e038834d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Are', 'there', 'any', 'whites', 'left', 'in', 'that', 'craphole', 'or', 'have', 'they', 'all', 'been', 'killed', '?']\n",
            "15\n"
          ]
        }
      ],
      "source": [
        "a_len=a.split()\n",
        "print(a_len)\n",
        "print(len(a_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NocRr4xtUNkT",
        "outputId": "1fd58478-cbae-4a1b-b258-9b0f4d12477e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Are there any whites left in that craphole or have they all been killed ?']"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RC7Fsk1-jjk"
      },
      "outputs": [],
      "source": [
        "outputs=tokenizer.texts_to_sequences(\"Are there any whites left in that craphole or have they all been killed ?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJkNRp0d_DMd"
      },
      "source": [
        "so if you input array then it works fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTDM9Vk7_BFU",
        "outputId": "ec0c8be4-9dca-46b0-9b31-197e3c797b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2],\n",
              " [6],\n",
              " [1],\n",
              " [],\n",
              " [3],\n",
              " [4],\n",
              " [1],\n",
              " [6],\n",
              " [1],\n",
              " [],\n",
              " [2],\n",
              " [7],\n",
              " [9],\n",
              " [],\n",
              " [11],\n",
              " [4],\n",
              " [8],\n",
              " [3],\n",
              " [1],\n",
              " [12],\n",
              " [],\n",
              " [5],\n",
              " [1],\n",
              " [13],\n",
              " [3],\n",
              " [],\n",
              " [8],\n",
              " [7],\n",
              " [],\n",
              " [3],\n",
              " [4],\n",
              " [2],\n",
              " [3],\n",
              " [],\n",
              " [14],\n",
              " [6],\n",
              " [2],\n",
              " [15],\n",
              " [4],\n",
              " [10],\n",
              " [5],\n",
              " [1],\n",
              " [],\n",
              " [10],\n",
              " [6],\n",
              " [],\n",
              " [4],\n",
              " [2],\n",
              " [16],\n",
              " [1],\n",
              " [],\n",
              " [3],\n",
              " [4],\n",
              " [1],\n",
              " [9],\n",
              " [],\n",
              " [2],\n",
              " [5],\n",
              " [5],\n",
              " [],\n",
              " [17],\n",
              " [1],\n",
              " [1],\n",
              " [7],\n",
              " [],\n",
              " [18],\n",
              " [8],\n",
              " [5],\n",
              " [5],\n",
              " [1],\n",
              " [19],\n",
              " [],\n",
              " [20]]"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "it5dXRYU-01l",
        "outputId": "aa022209-4ae1-4f44-f62b-7c084361c5d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBl9M2cb-20z",
        "outputId": "42840417-3923-4619-863e-56095669835e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "['are', 'there', 'any', 'whites', 'left', 'in', 'that', 'craphole', 'or', 'have', 'they', 'all', 'been', 'killed', '?']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a TensorFlow Tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='[OOV]')\n",
        "\n",
        "# Fit the tokenizer on the input text\n",
        "tokenizer.fit_on_texts([a])\n",
        "\n",
        "def custom_tokenizer(input_text):\n",
        "\n",
        "\n",
        "    # Get the word index from the tokenizer\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    # Tokenize the input text using the tokenizer\n",
        "    tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    print(tokens)\n",
        "    # Convert tokens back to words, punctuation, and special characters\n",
        "    reverse_word_index = {v: k for k, v in word_index.items()}\n",
        "    tokens = [reverse_word_index[token] for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "tokens = custom_tokenizer(a)\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf-WLv73ZZqN",
        "outputId": "016efded-681e-428c-a8f9-62bf7d2f5153"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-116-5eabf16cdd78>:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  testing=pd.Series()\n"
          ]
        }
      ],
      "source": [
        "testing=pd.Series()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFVNT8p2a8Ux"
      },
      "outputs": [],
      "source": [
        "testing.at[0]=\"hey whites left here\"\n",
        "testing.at[1]=\"are jsdklfjh in that craphole\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGnlufbDa-lg",
        "outputId": "fb7ee10e-6201-4e83-ffee-49b6073ebef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['hey whites left here', 'are jsdklfjh in that craphole'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFu2w3E7bQVx"
      },
      "outputs": [],
      "source": [
        "output=tokenizer.texts_to_sequences(testing.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO3dAlGhbeYY",
        "outputId": "8582c079-dc4c-47fa-885c-83cd8c847099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 5, 6, 1], [2, 1, 7, 8, 9]]"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOgBFE1FbfHJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}